{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft AI Challenge\n",
    "\n",
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output as clr\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer as WNL\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'Datasets',\n",
       " 'dev-evaluate-v2.0-in1.json',\n",
       " 'Embeddings',\n",
       " 'evaluate-v2.0.py',\n",
       " 'Models',\n",
       " 'Readings',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Working Dircetory\n",
    "cw_path = os.getcwd() + '/../../'\n",
    "os.listdir(cw_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "    \n",
    "    def __init__(self, mqw, mpw):\n",
    "        \n",
    "        self.GloveEmbeddings = {}\n",
    "        self.emb_dim = 50\n",
    "        self.max_query_words = mqw\n",
    "        self.max_passage_words = mpw\n",
    "        \n",
    "        self.lemmatizer = WNL()\n",
    "    \n",
    "    \n",
    "    def normalize_text(self, s):\n",
    "        lower_s = self.lower(s)\n",
    "        rem_p_s = self.remove_punc(lower_s)\n",
    "        rem_a_s = self.remove_articles(rem_p_s)\n",
    "        space_s = self.white_space_fix(rem_a_s)\n",
    "        lemma_s = self.lemmatize(space_s)\n",
    "        return lemma_s\n",
    "        \n",
    "    def lemmatize(self, txt):\n",
    "        lemmatizer = self.lemmatizer\n",
    "        return ' '.join(lemmatizer.lemmatize(lemmatizer.lemmatize(word, pos = 'v'), pos = 'v') for word in txt.split())\n",
    "\n",
    "    def remove_articles(self, text):\n",
    "        return re.sub('\\s+(a|an|and|the)(\\s+)', ' ',text)\n",
    "\n",
    "    def white_space_fix(self, text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(self, text):\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        return text.translate(translator)\n",
    "\n",
    "    def lower(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def loadEmbeddings(self, embeddingfile):\n",
    "\n",
    "        fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "        for line in fe:\n",
    "            tokens= line.strip().split()\n",
    "            word = tokens[0]\n",
    "            vec = tokens[1:]\n",
    "            vec = \" \".join(vec)\n",
    "            self.GloveEmbeddings[word]=vec\n",
    "        self.emb_dim = len(vec.split(' '))\n",
    "        #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "        self.GloveEmbeddings[\"zerovec\"] = \"0.0 \"*(self.emb_dim-1) + \"0.0\"\n",
    "        fe.close()\n",
    "        \n",
    "    def mini_df_to_stacks(self, df):\n",
    "        try: \n",
    "            df = df[df[0] != 0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df_list = []\n",
    "        que_stack = []\n",
    "        ans_stack = []\n",
    "        out_stack = []\n",
    "        ratio_stack = []\n",
    "        for ind in range(df.shape[0]):\n",
    "            vals = df.values\n",
    "            question = self.normalize_text(vals[ind][1]).strip().split()\n",
    "            new_question = []\n",
    "            for word in question:\n",
    "                if(not word):\n",
    "                    continue\n",
    "                try:\n",
    "                    new_question.append(np.array(self.GloveEmbeddings[word].strip().split()).astype(float))\n",
    "                except:\n",
    "                    new_question.append(np.array(self.GloveEmbeddings[\"zerovec\"].strip().split()).astype(float))\n",
    "            answer = self.normalize_text(vals[ind][2]).strip().split()\n",
    "            new_answer = []\n",
    "            for word in answer:\n",
    "                if(not word):\n",
    "                    continue\n",
    "                try:\n",
    "                    new_answer.append(np.array(self.GloveEmbeddings[word].strip().split()).astype(float))\n",
    "                except:\n",
    "                    new_answer.append(np.array(self.GloveEmbeddings[\"zerovec\"].strip().split()).astype(float))\n",
    "                    \n",
    "            \n",
    "            l = vals[ind][3]\n",
    "            que = np.array(new_question)\n",
    "            ans = np.array(new_answer)\n",
    "            que = que[:min(self.max_query_words, que.shape[0])]\n",
    "            ans = ans[:min(self.max_passage_words, ans.shape[0])]\n",
    "            \n",
    "            stems_a = question\n",
    "            stems_b = answer\n",
    "            ratio = len(set(stems_a).intersection(stems_b)) / float(len(set(stems_a)))\n",
    "            \n",
    "            zec = np.array(self.GloveEmbeddings[\"zerovec\"].strip().split()).astype(float).reshape(1,self.emb_dim)\n",
    "            \n",
    "            try:\n",
    "                pad_que = np.concatenate([que, np.tile(zec.reshape(self.emb_dim,1), \n",
    "                                                       self.max_query_words - que.shape[0]).T], axis = 0\n",
    "                                        ).reshape(self.max_query_words,self.emb_dim)\n",
    "\n",
    "                pad_ans = np.concatenate([ans, np.tile(zec.reshape(self.emb_dim,1), \n",
    "                                                       self.max_passage_words - ans.shape[0]).T], axis = 0\n",
    "                                        ).reshape(self.max_passage_words,self.emb_dim)\n",
    "\n",
    "                final_out = np.array([1,0]).reshape(-1)\n",
    "                if(l==1):\n",
    "                    final_out = np.array([0,1]).reshape(-1)\n",
    "\n",
    "\n",
    "                que_stack.append(pad_que)\n",
    "                ans_stack.append(pad_ans)\n",
    "                out_stack.append(final_out)\n",
    "                ratio_stack.append(ratio)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "        \n",
    "        que_stack = np.array(que_stack)\n",
    "        ans_stack = np.array(ans_stack)\n",
    "        out_stack = np.array(out_stack)\n",
    "        ratio_stack = np.array(ratio_stack)\n",
    "        \n",
    "        return (que_stack, ans_stack, out_stack, ratio_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing preprocessor\n",
    "q_max, a_max = 40,120\n",
    "\n",
    "PRE = preprocessor(q_max, a_max)\n",
    "embeddingFileName = cw_path + \"Embeddings/glove.6B.100d.txt\"\n",
    "PRE.loadEmbeddings(embeddingFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brilliant i know somebody be go to submit regex as solution i m afraid i m amateur though do you know of any good regex tutorials reference\n"
     ]
    }
   ],
   "source": [
    "### Check if its working ----\n",
    "text = \"Brilliant! I knew somebody was going to submit regex as the solution. I'm afraid I'm an amateur, though. Do you know of any good regex tutorials/references?\"\n",
    "print(PRE.normalize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "logs_path = cw_path + 'Models/Model_01/maic2'\n",
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input params\n",
    "\n",
    "## Question module params\n",
    "question_shape = (q_max, PRE.emb_dim)\n",
    "q_hidden_units = PRE.emb_dim\n",
    "\n",
    "## Answer module params\n",
    "answer_shape = (a_max, PRE.emb_dim)\n",
    "a_hidden_units = PRE.emb_dim\n",
    "\n",
    "## Episoidic memory params\n",
    "episode_shape = (a_max, 25)\n",
    "e_hidden_units = PRE.emb_dim\n",
    "n_episodes = 5\n",
    "\n",
    "## Output memory params\n",
    "output_shape = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class question_module():\n",
    "    def __init__(self,input_shape, q_hidden_size, question_ph, sess):\n",
    "        with tf.name_scope('Q_module') as scope:\n",
    "            with sess.as_default():\n",
    "\n",
    "                self.n_steps, self.emb_dim = input_shape\n",
    "\n",
    "                self.q_hidden_size = q_hidden_size\n",
    "\n",
    "                self.question = question_ph\n",
    "\n",
    "                self.q_cell = tf.nn.rnn_cell.LSTMCell(q_hidden_size, initializer=tf.variance_scaling_initializer(), name = 'Question_Encoder_Cell')\n",
    "\n",
    "                self.init_s, self.init_c = self.q_cell.zero_state(tf.shape(self.question)[0], tf.float32)\n",
    "\n",
    "                self.s = self.init_s\n",
    "                self.c = self.init_c\n",
    "\n",
    "                self.state = [self.s, self.c]\n",
    "\n",
    "                for i in range(self.n_steps):\n",
    "                    self.output, self.state = self.q_cell(self.question[:,i,:], state = self.state)\n",
    "\n",
    "\n",
    "                self.final_state = self.state\n",
    "\n",
    "    def encode(self):\n",
    "        return self.final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class answer_module():\n",
    "    def __init__(self,input_shape, a_hidden_size, answer_ph, sess):\n",
    "        with tf.name_scope('A_module') as scope:\n",
    "            with sess.as_default():\n",
    "\n",
    "                self.n_steps, self.emb_dim = input_shape\n",
    "\n",
    "                self.a_hidden_size = a_hidden_size\n",
    "\n",
    "                self.answer = answer_ph\n",
    "\n",
    "                self.a_cell = tf.nn.rnn_cell.LSTMCell(a_hidden_size, initializer=tf.variance_scaling_initializer(), name = 'Answer_Encoder_Cell')\n",
    "\n",
    "                self.init_s, self.init_c = self.a_cell.zero_state(tf.shape(self.answer)[0], tf.float32)\n",
    "\n",
    "                self.s = self.init_s\n",
    "                self.c = self.init_c\n",
    "\n",
    "                self.state = [self.s, self.c]\n",
    "\n",
    "                self.outputs = []\n",
    "                for i in range(self.n_steps):\n",
    "                    self.output, self.state = self.a_cell(self.answer[:,i,:], state = self.state)\n",
    "                    self.outputs.append(self.output)\n",
    "\n",
    "                self.final_state = self.state\n",
    "                self.outputs = tf.stack(self.outputs, axis = 1, name = 'Answers_Encoded_Stacked')\n",
    "\n",
    "\n",
    "    def encode(self):\n",
    "        return self.outputs, self.final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class episoidic_mem_module():\n",
    "    \n",
    "    def __init__(self,input_shape, e_hidden_size, depth_mem, episode_ph, quec_ph, queh_ph, sess):\n",
    "        with tf.name_scope('E_module') as scope:\n",
    "            with sess.as_default():\n",
    "\n",
    "                self.n_steps, self.emb_dim = input_shape\n",
    "\n",
    "                self.e_hidden_size = e_hidden_size\n",
    "\n",
    "                self.cepisode_ans = episode_ph\n",
    "                self.question_enc_c = quec_ph\n",
    "                self.question_enc_h = queh_ph\n",
    "\n",
    "                self.que_enc = self.question_enc_c\n",
    "                self.episode = self.cepisode_ans\n",
    "\n",
    "\n",
    "                self.outputs = []\n",
    "                for k in range(depth_mem):\n",
    "                    sequence = self.episode\n",
    "                    state_prev = self.que_enc\n",
    "                    attension_k = self.get_attensions(state_prev, sequence, k)\n",
    "                    self.que_enc = attension_k[:,0,:]\n",
    "\n",
    "                    self.outputs.append(attension_k)\n",
    "                    \n",
    "                self.outputs = tf.concat(self.outputs, axis = 1, name = 'Episodes_Encoded_Stacked')\n",
    "                print(self.outputs.shape)\n",
    "\n",
    "                self.fcell = tf.nn.rnn_cell.LSTMCell(e_hidden_size, initializer=tf.variance_scaling_initializer(), name = 'Epsiodic_Final_LSTM')\n",
    "                self.fstate = [self.question_enc_c, self.question_enc_h]\n",
    "                for k in range(depth_mem):\n",
    "                    out, self.fstate = self.fcell(self.outputs[:,k,:], state = self.fstate)\n",
    "\n",
    "                self.out = out\n",
    "            \n",
    "    def get_attensions(self, state_prev, sequence, k):\n",
    "        with tf.name_scope('E_module_attensions') as scope:\n",
    "            N = sequence.shape[1]\n",
    "            s_new = tf.keras.backend.repeat(state_prev,N)\n",
    "            a_new = sequence#tf.stack(sequence, axis = 1)\n",
    "            concat = tf.concat([s_new, a_new], axis = -1, name = 'Attenssion_Concat_' + str(k))\n",
    "            dense1 = tf.layers.dense(concat, 100, activation=tf.nn.leaky_relu, name = 'Attenssion_Dense1_' + str(k))\n",
    "            batch1 = tf.layers.batch_normalization(dense1, momentum=0.4, training=True)\n",
    "            drop1 = tf.layers.dropout(batch1, rate=0.4, training=True)\n",
    "            dense2 = tf.layers.dense(drop1, 10, activation=tf.nn.leaky_relu, name = 'Attenssion_Dense2_' + str(k))\n",
    "            batch2 = tf.layers.batch_normalization(dense2, momentum=0.4, training=True)\n",
    "            drop2 = tf.layers.dropout(batch2, rate=0.4, training=True)\n",
    "            dense3 = tf.layers.dense(drop2, 1, activation=tf.nn.leaky_relu, name = 'Attenssion_Dense3_' + str(k))\n",
    "            alphas = tf.nn.softmax(dense3, axis = 1, name = 'Attenssion_Alphas_' + str(k))\n",
    "            context = tf.keras.layers.Dot(axes = 1)([alphas, a_new])\n",
    "            return context\n",
    "    \n",
    "    \n",
    "    def episodic_enc(self):\n",
    "            return self.fstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class final_model:\n",
    "    \n",
    "    def __init__(self,sess):\n",
    "        with sess.as_default():\n",
    "            \n",
    "            self.sess = sess\n",
    "            \n",
    "            with tf.name_scope('PHs') as scope:\n",
    "                self.question_n = tf.placeholder(tf.float32, [None, question_shape[0], question_shape[1]], name = 'Que_PH')\n",
    "                self.answer_n = tf.placeholder(tf.float32, [None, answer_shape[0], answer_shape[1]],name = 'Ans_PH')\n",
    "                self.label = tf.placeholder(tf.float32, [None, output_shape], name = 'Lab_PH')\n",
    "                self.sample_weights = tf.placeholder(tf.float32, [None,], name = 'sample_weights')\n",
    "                self.ratio = tf.placeholder(tf.float32, [None,1], name = 'Ratios')\n",
    "                \n",
    "                batch_que = tf.layers.batch_normalization(self.question_n, momentum=0.4, training=True)\n",
    "                batch_ans = tf.layers.batch_normalization(self.answer_n, momentum=0.4, training=True)\n",
    "                \n",
    "                self.question = batch_que\n",
    "                self.answer = batch_ans\n",
    "                \n",
    "                print(self.question.shape, self.answer.shape)\n",
    "            \n",
    "            with tf.name_scope('Modules') as scope:\n",
    "                self.Q_module = question_module(question_shape, q_hidden_units,self.question, sess)\n",
    "                self.A_module = answer_module(answer_shape, a_hidden_units,self.answer, sess)\n",
    "\n",
    "                self.encoding_q_n = self.Q_module.encode()\n",
    "                self.a_outputs_n, self.encoding_a_n = self.A_module.encode()\n",
    "                \n",
    "                print(self.encoding_q_n.c.shape, self.a_outputs_n.shape)\n",
    "                \n",
    "                batch_que_enc_c = tf.layers.batch_normalization(self.encoding_q_n.c, momentum=0.4, training=True)\n",
    "                batch_que_enc_h = tf.layers.batch_normalization(self.encoding_q_n.h, momentum=0.4, training=True)\n",
    "                batch_ans_out = tf.layers.batch_normalization(self.a_outputs_n, momentum=0.4, training=True)\n",
    "                \n",
    "                self.a_outputs = batch_ans_out\n",
    "\n",
    "                self.E_module = episoidic_mem_module(episode_shape, e_hidden_units, n_episodes,self.a_outputs,batch_que_enc_c, batch_que_enc_h, sess)\n",
    "\n",
    "                self.e_outputs_n = self.E_module.episodic_enc()\n",
    "                \n",
    "                batch_epi_enc_c = tf.layers.batch_normalization(self.e_outputs_n.c, momentum=0.4, training=True)\n",
    "                batch_epi_enc_h = tf.layers.batch_normalization(self.e_outputs_n.h, momentum=0.4, training=True)\n",
    "                \n",
    "            \n",
    "            with tf.name_scope('Final') as scope:\n",
    "                prevec = tf.concat([batch_epi_enc_c, batch_que_enc_c], axis =1, name = 'QUE_EPI_concat')\n",
    "                dense = tf.layers.dense(prevec, 16, activation=tf.nn.leaky_relu, name = 'Final_Dense')\n",
    "                concat1 = tf.concat([dense, self.ratio], axis = 1, name = 'ratio_concat')\n",
    "                dense1 = tf.layers.dense(concat1, 8, activation=tf.nn.leaky_relu, name = 'Final_Dense1')\n",
    "                dense2 = tf.layers.dense(concat1, 8, activation=tf.nn.leaky_relu, name = 'Final_Dense2')\n",
    "                self.logits = tf.layers.dense(dense1, output_shape, name = 'Final_Logits')\n",
    "                self.perf = tf.layers.dense(dense2, 1, name = 'Final_Perf')\n",
    "\n",
    "                self.out = tf.nn.softmax(self.logits, name  = 'Final_Out')\n",
    "            \n",
    "            with tf.name_scope('Training') as scope:\n",
    "                self.loss_vec = tf.nn.softmax_cross_entropy_with_logits_v2(labels = tf.stop_gradient(self.label), logits = self.logits, name = 'Final_Loss')\n",
    "                self.c_perf = tf.reduce_sum(tf.multiply(self.perf, self.sample_weights))\n",
    "                self.v_loss = tf.reduce_sum(tf.multiply(self.loss_vec, self.sample_weights))\n",
    "                self.loss = (self.v_loss-tf.stop_gradient(self.c_perf))\n",
    "                self.loss2 = tf.square(tf.stop_gradient(self.loss) - self.c_perf)\n",
    "                \n",
    "                self.opt = tf.train.AdamOptimizer(0.0002, name = 'Optimizer')\n",
    "                self.opt2 = tf.train.AdamOptimizer(0.0002, name = 'Optimizer2')\n",
    "                \n",
    "                self.train_step = self.opt.minimize(self.loss)\n",
    "                self.train_Step2 = self.opt2.minimize(self.loss2)\n",
    "                \n",
    "#                 self.grads = self.opt.compute_gradients(self.loss)\n",
    "#                 self.grads = [(tf.clip_by_value(g, -1.0, 1.0), v) for g, v in self.grads]\n",
    "#                 self.train_step = self.opt.apply_gradients(self.grads)\n",
    "            \n",
    "            with tf.name_scope('Init') as scope:\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "                self.init_vars = tf.variables_initializer(self.variables)\n",
    "            \n",
    "    def train(self, question, answer, label, ratio,sample_weights=None, epochs = 1, verbose = 1):\n",
    "        sum_sw = np.sum(sample_weights)\n",
    "        N = sample_weights.shape[0]\n",
    "        sample_weights = N*sample_weights/sum_sw\n",
    "        if(sample_weights is None):\n",
    "            sample_weights = np.ones((label.shape[0]))\n",
    "        _feed_dict = {self.ratio : ratio, self.question_n: question, self.answer_n : answer,  self.label :label, self.sample_weights : sample_weights}\n",
    "        for i in range(epochs):\n",
    "            self.sess.run([self.train_step], feed_dict = _feed_dict)\n",
    "        self.sess.run([self.train_Step2], feed_dict = _feed_dict)\n",
    "        if(verbose == 1):\n",
    "            loss, out, logits = self.sess.run([self.v_loss, self.out, self.logits], feed_dict = _feed_dict)\n",
    "            print('Epochs :',i+1,', loss:', loss, ', accuracy:', self.accuracy(label, out, sample_weights))\n",
    "            return logits\n",
    "\n",
    "    def accuracy(self, y_true, y_pred, sample_weights):\n",
    "        y_tl = np.argmax(y_true, axis = 1)\n",
    "        y_pr = np.argmax(y_pred, axis = 1)\n",
    "        acc1 = np.mean((y_tl == y_pr)*sample_weights.reshape(-1,1))\n",
    "        acc2 = np.mean((y_tl == y_pr))\n",
    "        acc = str(acc1)+'::'+str(acc2)\n",
    "        return acc\n",
    "    \n",
    "#     def predict(self, question , answer):\n",
    "#         _feed_dict = {self.question_n: question, self.answer_n : answer,  self.label :label}\n",
    "#         out = self.sess.run([self.out], feed_dict = _feed_dict)[0]\n",
    "#         return np.argmax(out, axis = 1)\n",
    "    \n",
    "    def score(self, question, answer, ratio):\n",
    "        _feed_dict = {self.question_n: question, self.answer_n : answer, self.ratio : ratio}\n",
    "        out = self.sess.run(self.out, feed_dict = _feed_dict)\n",
    "        scores = out[:,1]\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "#     def make_partial_graph(self):\n",
    "        \n",
    "#         with tf.name_scope('partial_Final') as scope:\n",
    "            \n",
    "#             self.pr_lr = tf.placeholder(tf.float32, shape =[], name = 'pr_lr')\n",
    "            \n",
    "#             batch_ans_enc_c = tf.layers.batch_normalization(self.encoding_a_n.c, momentum=0.4, training=True)\n",
    "#             batch_epi_enc_c = tf.layers.batch_normalization(self.e_outputs_n.c, momentum=0.4, training=True)\n",
    "#             batch_que_enc_c = tf.layers.batch_normalization(self.encoding_q_n.c, momentum=0.4, training=True)\n",
    "            \n",
    "            \n",
    "#             prevec = tf.concat([batch_epi_enc_c, batch_que_enc_c, batch_ans_enc_c], axis =1, name = 'pr_QUE_EPI_concat')\n",
    "#             dense = tf.layers.dense(prevec, 32, activation=tf.nn.leaky_relu, name = 'pr_Final_Dense')\n",
    "#             concat1 = tf.concat([dense, self.ratio], axis = 1, name = 'pr_ratio_concat')\n",
    "#             dense1 = tf.layers.dense(concat1, 16, activation=tf.nn.leaky_relu, name = 'pr_Final_Dense1')\n",
    "#             self.pr_logits = tf.layers.dense(dense1, output_shape, name = 'pr_Final_Logits')\n",
    "#             self.pr_out = tf.nn.softmax(self.pr_logits, name  = 'pr_Final_Out')\n",
    "        \n",
    "#         with tf.name_scope('partial_Training') as scope:\n",
    "            \n",
    "# #             self.pr_loss_vec = tf.nn.softmax_cross_entropy_with_logits_v2(labels = tf.stop_gradient(self.label),\n",
    "# #                                                                        logits = self.pr_logits, name = 'partial_Final_Loss')\n",
    "            \n",
    "            \n",
    "            \n",
    "#             self.pr_loss_vec = tf.losses.hinge_loss(labels = tf.stop_gradient(self.label),\n",
    "#                                                                        logits = self.pr_logits, reduction = tf.losses.Reduction.NONE)\n",
    "            \n",
    "#             self.pr_v_loss = tf.reduce_sum(tf.multiply(tf.reduce_sum(self.pr_loss_vec, axis  = 1), self.sample_weights))\n",
    "\n",
    "#             self.pr_opt = tf.train.AdamOptimizer(learning_rate = self.pr_lr, name = 'partial_Optimizer')\n",
    "                \n",
    "# #             self.pr_grads = self.pr_opt.compute_gradients(self.pr_v_loss)\n",
    "# #             self.pr_grads = [(tf.clip_by_value(g, -1.0, 1.0), v) for g, v in self.pr_grads]\n",
    "# #             self.pr_train_step = self.opt.apply_gradients(self.pr_grads)\n",
    "            \n",
    "#             self.pr_train_step = self.pr_opt.minimize(self.pr_v_loss)\n",
    "            \n",
    "#         with tf.name_scope('partial_Init') as scope:\n",
    "#             self.pr_vars = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "#             self.pr_variables = [v for v in self.pr_vars if v not in self.variables]\n",
    "#             self.pr_init_vars = tf.variables_initializer(self.pr_variables)\n",
    "    \n",
    "    \n",
    "#     def pr_train(self, question, answer, label, ratio,sample_weights=None, epochs = 1, verbose = 1, lr = 0.01):\n",
    "#         sum_sw = np.sum(sample_weights)\n",
    "#         N = sample_weights.shape[0]\n",
    "#         sample_weights = N*sample_weights/sum_sw\n",
    "#         if(sample_weights is None):\n",
    "#             sample_weights = np.ones((label.shape[0]))\n",
    "#         _feed_dict = {self.ratio : ratio, self.question_n: question, self.answer_n : answer,\n",
    "#                       self.label :label, self.sample_weights : sample_weights, self.pr_lr : lr}\n",
    "        \n",
    "#         for i in range(epochs):\n",
    "#             self.sess.run([self.pr_train_step], feed_dict = _feed_dict)\n",
    "            \n",
    "#         if(verbose == 1):\n",
    "#             loss, out, logits = self.sess.run([self.pr_v_loss, self.pr_out, self.pr_logits], feed_dict = _feed_dict)\n",
    "#             print('Epochs :',i+1,', loss:', loss, ', accuracy:', self.accuracy(label, out, sample_weights))\n",
    "#             return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 40, 100) (?, 120, 100)\n",
      "(?, 100) (?, 120, 100)\n",
      "(?, 5, 100)\n"
     ]
    }
   ],
   "source": [
    "model = final_model(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.make_partial_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves2/episoid_mem_model\n"
     ]
    }
   ],
   "source": [
    "save_path = './saves2'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(model.sess,tf.train.latest_checkpoint(save_path))\n",
    "# model.sess.run(model.init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.make_partial_graph()\n",
    "\n",
    "# model.sess.run(model.pr_init_vars)\n",
    "\n",
    "# saver.save(model.sess, save_path+'/episoid_mem_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_loss = tf.summary.scalar(name='Avg_Loss_act', tensor=model.v_loss)\n",
    "# loss1 = tf.summary.scalar(name='Avg_Loss_1', tensor=model.loss)\n",
    "# loss2 = tf.summary.scalar(name='Avg_Loss_2', tensor=model.loss2)\n",
    "\n",
    "# pr_avg_loss = tf.summary.scalar(name='Pr_Avg_Loss_act', tensor=model.pr_v_loss)\n",
    "# # loss1 = tf.summary.scalar(name='Avg_Loss_1', tensor=model.loss)\n",
    "# # loss2 = tf.summary.scalar(name='Avg_Loss_2', tensor=model.loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csize = 1000\n",
    "# dfs = pd.read_csv(data_dir + 'data.tsv', sep='\\t', chunksize=csize, header = None)\n",
    "# count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in dfs:\n",
    "#     try: \n",
    "#         df = df[df[0] != 0]\n",
    "#     except:\n",
    "#         pass\n",
    "#     if(count*csize < 250*1000):\n",
    "#         print(count*csize)\n",
    "#         count +=1\n",
    "#         if(count%10 == 0):\n",
    "#             clr()\n",
    "#         continue\n",
    "        \n",
    "#     question, answer, label, ratio = PRE.mini_df_to_stacks(df)\n",
    "#     r_max = np.max(ratio)\n",
    "#     r_min = np.min(ratio)\n",
    "#     ratio_sw = (ratio-r_min)/(r_max-r_min)\n",
    "    \n",
    "#     ones = np.argmax(label, axis = 1)\n",
    "#     sample_weights_ratio1 = ones*ratio_sw\n",
    "    \n",
    "#     sample_weights_ratio2 = (1-ones)*(1-ratio_sw)\n",
    "#     sample_weights = (sample_weights_ratio1 + sample_weights_ratio2)*(0.8*ones + 0.1)\n",
    "   \n",
    "    \n",
    "    \n",
    "#     ratio = ratio.reshape(-1,1)\n",
    "#     if(count%10 == 0):\n",
    "#         y_label = np.argmax(label, axis = 1)\n",
    "#         idx_1 = np.where(y_label == 1)[0]\n",
    "#         idx_0 = np.where(y_label == 0)[0]\n",
    "#         sam_1 = np.random.choice(idx_1)\n",
    "#         sam_0 = np.random.choice(idx_0)\n",
    "\n",
    "#         dat_1 = df.iloc[sam_1]\n",
    "#         dat_0 = df.iloc[sam_0]\n",
    "        \n",
    "#         clr()\n",
    "#         print(count)\n",
    "#         logits = model.pr_train(question, answer, label, ratio, sample_weights, epochs = 1, verbose = 1, lr = 0.01)\n",
    "#         print(dat_1[0],':',dat_1[1],':',dat_1[2],':',dat_1[3], ': score :', logits[sam_1])\n",
    "#         print(dat_0[0],':',dat_0[1],':',dat_0[2],':',dat_0[3], ': score :', logits[sam_0])\n",
    "#         saver.save(model.sess, save_path+'/episoid_mem_model')\n",
    "#         if(count%50==0):\n",
    "#             txtrt = str(count) + \",\" + str(csize)\n",
    "#             with open('count.txt', 'w') as f:\n",
    "#                 f.write(txtrt)\n",
    "#                 f.close()\n",
    "#     else:\n",
    "#         print(count)\n",
    "#         model.pr_train(question, answer, label, ratio, sample_weights, epochs = 1, verbose = 0, lr = 0.01)\n",
    "        \n",
    "\n",
    "#     s0,s1,s2,s3 = model.sess.run([pr_avg_loss, avg_loss, loss1, loss2], feed_dict = {model.ratio : ratio,model.question_n: question,\n",
    "#                                                       model.answer_n : answer,\n",
    "#                                                       model.label :label,\n",
    "#                                                       model.sample_weights : sample_weights})\n",
    "#     writer.add_summary(s0, count)\n",
    "#     writer.add_summary(s1, count)\n",
    "#     writer.add_summary(s2, count)\n",
    "#     writer.add_summary(s3, count)\n",
    "        \n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(Question):\n",
    "    \n",
    "\n",
    "    search_url = \"https://en.wikipedia.org/w/index.php?search=\" + Question.replace(\" \", \"+\")\n",
    "\n",
    "    html = urllib.request.urlopen(search_url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    links = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            link = soup.findAll(\"div\", [\"mw-search-result-heading\"])[i].find('a')['href']\n",
    "            links.append('https://en.wikipedia.org' + link)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    net_text = 'Nothing here :)\\n'\n",
    "\n",
    "    for link in links:\n",
    "\n",
    "        html_link = urllib.request.urlopen(link).read()\n",
    "        soup_link = BeautifulSoup(html_link)\n",
    "\n",
    "        # kill all script and style elements\n",
    "        for script in soup_link([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup_link.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        if(len(text) > 1):\n",
    "            net_text += text + \"\\n\"\n",
    "\n",
    "    net_text+=\"Nothing here :)\"\n",
    "\n",
    "    list_sentences = net_text.split(\"\\n\")\n",
    "\n",
    "    answers = pd.DataFrame(list_sentences, columns = [2])\n",
    "\n",
    "    answers[1] = Question\n",
    "    answers[0] = 999\n",
    "    answers[3] = 0\n",
    "\n",
    "    df = answers.reindex(sorted(answers.columns), axis=1)\n",
    "\n",
    "    question, answer, label, ratio = PRE.mini_df_to_stacks(df.iloc[:-2])\n",
    "\n",
    "    scores = model.score(question, answer, ratio.reshape(-1,1))\n",
    "\n",
    "    index = np.argmax(scores)\n",
    "    \n",
    "    if(scores[index] > 0.7):\n",
    "        return list_sentences[np.argmax(scores)] + \", with confidence : \" + str(scores[index])\n",
    "    else:\n",
    "        return \"Sorry Not Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Search\n",
    "Question = \"Who is the father of the Computer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\",[17] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[18][19], with confidence : 0.8459683'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_answer(Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
